{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f34446b-62b1-4f96-ba5f-b6fc590c431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To unpack zip files\n",
    "import zipfile\n",
    "\n",
    "zip_file_path = '/home/said_harb_uri_edu/download.zip'  # Replace with the path to your zip file\n",
    "extract_path = '/home/said_harb_uri_edu/own_pics'  # Replace with the directory where you want to extract the files\n",
    "\n",
    "# Create a ZipFile object\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents of the zip file to the extraction directory\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf56fd8-144d-4697-ba37-a56251e84182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression complete. Zip file saved as compressed_images.zip\n"
     ]
    }
   ],
   "source": [
    "# #To create a zip file\n",
    "# import os\n",
    "# from zipfile import ZipFile\n",
    "\n",
    "# def compress_to_zip(directory_path, zip_filename):\n",
    "#     with ZipFile(zip_filename, 'w') as zipf:\n",
    "#         for root, dirs, files in os.walk(directory_path):\n",
    "#             for file in files:\n",
    "#                 if file.endswith(\".png\"):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     arcname = os.path.relpath(file_path, directory_path)\n",
    "#                     zipf.write(file_path, arcname=arcname)\n",
    "\n",
    "\n",
    "# # Specify the directory containing the .png files\n",
    "# input_directory = \"/work/pi_noah_daniels_uri_edu/said_harb_uri_edu_data/segmented_pictures/\"\n",
    "\n",
    "# # Specify the name of the output zip file\n",
    "# output_zip_filename = \"compressed_images.zip\"\n",
    "\n",
    "# # Compress the .png files into a zip archive\n",
    "# compress_to_zip(input_directory, output_zip_filename)\n",
    "\n",
    "# print(f\"Compression complete. Zip file saved as {output_zip_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8cbb01-3980-4e37-8802-1046296c6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To show an image of unfiltered COCO dataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# # Path to your JPG file\n",
    "# image_path = '/work/pi_noah_daniels_uri_edu/said_harb_uri_edu_data/COCO/train2017/000000000078.jpg'\n",
    "\n",
    "# # Read the image\n",
    "# img = mpimg.imread(image_path)\n",
    "\n",
    "# # Display the image\n",
    "# imgplot = plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83735338-212d-4ede-a4d3-f00abcebbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To filter COCO datast to have no instances which also appear in Cityscapes\n",
    "# import os\n",
    "# import sys\n",
    "# import time\n",
    "# import numpy as np\n",
    "# sys.path.append(os.path.dirname(sys.path[0]))\n",
    "\n",
    "# from PIL import Image\n",
    "# %run meta_ood/meta_ood_master/config\n",
    "# import meta_ood.meta_ood_master.config as my_config\n",
    "# %run meta_ood/meta_ood_master/src/dataset/coco\n",
    "# import meta_ood.meta_ood_master.src.dataset.coco as coco_tools\n",
    "# print(\"Cityscapes Root:\", cs_coco_roots.cs_root)\n",
    "# from pycocotools.coco import COCO as coco_tools\n",
    "# def main():\n",
    "#     start = time.time()\n",
    "#     root = cs_coco_roots.coco_root\n",
    "#     split = \"train\"\n",
    "#     year = 2017\n",
    "#     id_in = COCO.train_id_in\n",
    "   \n",
    "#     id_out = COCO.train_id_out\n",
    "#     min_size = COCO.min_image_size\n",
    "\n",
    "#     annotation_file = '{}/annotations/instances_{}.json'.format(root, split+str(year))\n",
    "#     print('Annotation file directory: ' +annotation_file)\n",
    "#     images_dir = '{}/{}'.format(root, split+str(year))\n",
    "#     print('Images directory: ' +images_dir)\n",
    "#     tools = coco_tools(annotation_file)\n",
    "#     save_dir = '{}/annotations/ood_seg_{}'.format(root, split+str(year))\n",
    "#     print('Save directory: '+save_dir)\n",
    "#     print(\"\\nPrepare COCO{} {} split for OoD training\".format(str(year), split))\n",
    "\n",
    "#     # Names of classes that are excluded - these are Cityscapes classes also available in COCO\n",
    "#     exclude_classes = ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'traffic light', 'stop sign']\n",
    "\n",
    "#     # Fetch all image ids that does not include instance from classes defined in \"exclude_classes\"\n",
    "#     exclude_cat_Ids = tools.getCatIds(catNms=exclude_classes)\n",
    "#     exclude_img_Ids = []\n",
    "#     for cat_Id in exclude_cat_Ids:\n",
    "#         exclude_img_Ids += tools.getImgIds(catIds=cat_Id)\n",
    "#     exclude_img_Ids = set(exclude_img_Ids)\n",
    "#     img_Ids = [int(image[:-4]) for image in os.listdir(images_dir) if int(image[:-4]) not in exclude_img_Ids]\n",
    "    \n",
    "#     print(\"Number of img_Ids: \"+str(len(img_Ids)))\n",
    "#     num_masks = 0\n",
    "#     # Process each image\n",
    "#     print(\"Ground truth segmentation mask will be saved in:\", save_dir)\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "#         print(\"Created save directory:\", save_dir)\n",
    "#     for i, img_Id in enumerate(img_Ids):\n",
    "#         img = tools.loadImgs(img_Id)[0]\n",
    "#         h, w = img['height'], img['width']\n",
    "\n",
    "#         # Select only images with height and width of at least min_size\n",
    "#         if h >= min_size and w >= min_size:\n",
    "#             ann_Ids = tools.getAnnIds(imgIds=img['id'], iscrowd=None)\n",
    "#             annotations = tools.loadAnns(ann_Ids)\n",
    "\n",
    "#             # Generate binary segmentation mask\n",
    "#             mask = np.ones((h, w), dtype=\"uint8\") * id_in\n",
    "#             for j in range(len(annotations)):\n",
    "#                 mask = np.maximum(tools.annToMask(annotations[j])*id_out, mask)\n",
    "\n",
    "#             # Save segmentation mask\n",
    "#             Image.fromarray(mask).save(os.path.join(save_dir, \"{:012d}.png\".format(img_Id)))\n",
    "#             num_masks += 1\n",
    "#         print(\"\\rImages Processed: {}/{}\".format(i + 1, len(img_Ids)), end=' ')\n",
    "#         sys.stdout.flush()\n",
    "\n",
    "#     # Print summary\n",
    "#     print(\"\\nNumber of created segmentation masks with height and width of at least %d pixels:\" % min_size, num_masks)\n",
    "#     end = time.time()\n",
    "#     hours, rem = divmod(end - start, 3600)\n",
    "#     minutes, seconds = divmod(rem, 60)\n",
    "#     print(\"FINISHED {:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e3ba3-3093-4778-a725-b9b2a4368108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To display a .json file\n",
    "# import json\n",
    "\n",
    "# file_path = '/work/pi_noah_daniels_uri_edu/said_harb_uri_edu_data/COCO/annotations/instances_val2017.json'\n",
    "\n",
    "# with open(file_path, 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Assuming 'images' is a key in your dictionary\n",
    "# if 'images' in data:\n",
    "#     first_few_images = data['images'][:10]\n",
    "#     print(first_few_images)\n",
    "# else:\n",
    "#     print(\"The key 'images' is not present in the loaded JSON data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e967f9-7ed9-4aa2-b806-771850b3853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To get the number of files in a directory\n",
    "# import os\n",
    "\n",
    "# def count_files(directory):\n",
    "#     # Get the list of files in the directory\n",
    "#     files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    \n",
    "#     # Get the count of files\n",
    "#     file_count = len(files)\n",
    "\n",
    "#     return file_count\n",
    "\n",
    "# # Specify the directory path\n",
    "# directory_path = '/work/pi_noah_daniels_uri_edu/said_harb_uri_edu_data/COCO/annotations/ood_seg_train2017'\n",
    "\n",
    "# # Get the number of files in the directory\n",
    "# num_files = count_files(directory_path)\n",
    "\n",
    "# # Print the result\n",
    "# print(f'Number of files in {directory_path}: {num_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ffa3f3-15c5-45ce-8326-e282ff181c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# #To download files\n",
    "# import requests\n",
    "\n",
    "# url = \"https://www.cityscapes-dataset.com/file-handling/?packageID=3\"  # Replace with the URL of the file you want to download\n",
    "# destination_path = \"/work/pi_noah_daniels_uri_edu/said_harb_uri_edu_data/cs.zip\"  # Replace with the local path where you want to save the file\n",
    "\n",
    "# try:\n",
    "#     # Send a GET request to the URL\n",
    "#     response = requests.get(url)\n",
    "    \n",
    "#     # Check if the request was successful (status code 200)\n",
    "#     if response.status_code == 200:\n",
    "#         # Open the local file in binary mode and write the content of the response\n",
    "#         with open(destination_path, 'wb') as file:\n",
    "#             file.write(response.content)\n",
    "#         print(\"File downloaded successfully.\")\n",
    "#     else:\n",
    "#         print(f\"Error: {response.status_code} - {response.reason}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff60e29-1a63-4adb-96d5-82ff54ce19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To crate _labelTrainIds.png\n",
    "# #!/usr/bin/python\n",
    "# #\n",
    "# # Converts the polygonal annotations of the Cityscapes dataset\n",
    "# # to images, where pixel values encode ground truth classes.\n",
    "# #\n",
    "# # The Cityscapes downloads already include such images\n",
    "# #   a) *color.png             : the class is encoded by its color\n",
    "# #   b) *labelIds.png          : the class is encoded by its ID\n",
    "# #   c) *instanceIds.png       : the class and the instance are encoded by an instance ID\n",
    "# # \n",
    "# # With this tool, you can generate option\n",
    "# #   d) *labelTrainIds.png     : the class is encoded by its training ID\n",
    "# # This encoding might come handy for training purposes. You can use\n",
    "# # the file labels.py to define the training IDs that suit your needs.\n",
    "# # Note however, that once you submit or evaluate results, the regular\n",
    "# # IDs are needed.\n",
    "# #\n",
    "# # Uses the converter tool in 'json2labelImg.py'\n",
    "# # Uses the mapping defined in 'labels.py'\n",
    "# #\n",
    "\n",
    "# # python imports\n",
    "# from __future__ import print_function, absolute_import, division\n",
    "# import os, glob, sys\n",
    "\n",
    "# # cityscapes imports\n",
    "# from cityscapes_scripts.cityscapesScripts_master.cityscapesscripts.helpers.csHelpers import printError\n",
    "# from cityscapes_scripts.cityscapesScripts_master.cityscapesscripts.preparation.json2labelImg import json2labelImg\n",
    "\n",
    "# # The main method\n",
    "\n",
    "# # Where to look for Cityscapes\n",
    "# # if 'CITYSCAPES_DATASET' in os.environ:\n",
    "# #     cityscapesPath = os.environ['CITYSCAPES_DATASET']\n",
    "# # else:\n",
    "# #     cityscapesPath = os.path.join(os.path.dirname(os.path.realpath(__file__)),'..','..')\n",
    "# cityscapesPath=\"/home/said_harb_uri_edu/cityscapes\"\n",
    "# # how to search for all ground truth\n",
    "# searchFine   = os.path.join( cityscapesPath , \"gtFine\"   , \"*\" , \"*\" , \"*_gt*_polygons.json\" )\n",
    "# # searchCoarse = os.path.join( cityscapesPath , \"gtCoarse\" , \"*\" , \"*\" , \"*_gt*_polygons.json\" )\n",
    "# print(os.path.join( cityscapesPath , \"gtFine\"   , \"*\" ))\n",
    "\n",
    "\n",
    "\n",
    "# gtFinePath = os.path.join(cityscapesPath, \"gtFine\")\n",
    "\n",
    "# # List all directories in gtFinePath\n",
    "# directories = [d for d in os.listdir(gtFinePath)]\n",
    "\n",
    "# print(directories)\n",
    "\n",
    "\n",
    "# # search files\n",
    "# filesFine = glob.glob( searchFine )\n",
    "# print(filesFine)\n",
    "# filesFine.sort()\n",
    "# # filesCoarse = glob.glob( searchCoarse )\n",
    "# # filesCoarse.sort()\n",
    "\n",
    "# # concatenate fine and coarse\n",
    "# files = filesFine# + filesCoarse\n",
    "# # files = filesFine # use this line if fine is enough for now.\n",
    "\n",
    "# # quit if we did not find anything\n",
    "# if not files:\n",
    "#     printError( \"Did not find any files. Please consult the README.\" )\n",
    "\n",
    "# # a bit verbose\n",
    "# print(\"Processing {} annotation files\".format(len(files)))\n",
    "\n",
    "# # iterate through files\n",
    "# progress = 0\n",
    "# print(\"Progress: {:>3} %\".format( progress * 100 / len(files) ), end=' ')\n",
    "# for f in files:\n",
    "#     # create the output filename\n",
    "#     dst = f.replace( \"_polygons.json\" , \"_labelTrainIds.png\" )\n",
    "\n",
    "#     # do the conversion\n",
    "#     try:\n",
    "#         json2labelImg( f , dst , \"trainIds\" )\n",
    "#     except:\n",
    "#         print(\"Failed to convert: {}\".format(f))\n",
    "#         raise\n",
    "\n",
    "#     # status\n",
    "#     progress += 1\n",
    "#     print(\"\\rProgress: {:>3} %\".format( progress * 100 / len(files) ), end=' ')\n",
    "#     sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc07f79-da6e-47d7-a94f-97d1ae536820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = np.array(img)\n",
    "# sem = np.array(sem)\n",
    "\n",
    "# # Display the images using matplotlib\n",
    "# for i in range(100,106):\n",
    "    \n",
    "#     img,sem=dataset_train[i]\n",
    "#     plt.figure()\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(\"RGB Image\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(sem)  # Assuming the 'sem' image is grayscale\n",
    "#     plt.title(\"Semantic Label Image\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87842f29-0937-4b95-b775-162411c2d57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 21:48:07.525030: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-18 21:48:08.757470: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 21:48:08.757591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 21:48:08.935196: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 21:48:09.377875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 21:48:14.012753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n"
     ]
    }
   ],
   "source": [
    "#To check how man GPU's are available\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a572b67d-7251-4bd7-b2da-36565793affe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12.1\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti, Total Memory: 10.75 GiB\n",
      "GPU 1: NVIDIA GeForce RTX 2080 Ti, Total Memory: 10.75 GiB\n",
      "GPU 2: NVIDIA GeForce RTX 2080 Ti, Total Memory: 10.75 GiB\n",
      "GPU 3: NVIDIA GeForce RTX 2080 Ti, Total Memory: 10.75 GiB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}, Total Memory: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GiB\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba8eefd-54c4-472c-be26-06529d09e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see how much disk space is in a directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def get_free_space(directory):\n",
    "    \"\"\"\n",
    "    Get free space in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory path.\n",
    "\n",
    "    Returns:\n",
    "        int: Free space in bytes.\n",
    "    \"\"\"\n",
    "    free_space = shutil.disk_usage(directory).free\n",
    "    return free_space\n",
    "\n",
    "# Specify the directory you want to check\n",
    "directory_to_check = '/work/pi_noah_daniels_uri_edu/said_harb_uri_edu_data'\n",
    "\n",
    "# Get and print the free space\n",
    "free_space_bytes = get_free_space(directory_to_check)\n",
    "print(f\"Free space in {directory_to_check}: {free_space_bytes} bytes\")\n",
    "\n",
    "# Convert bytes to GB for easier readability\n",
    "free_space_gb = free_space_bytes / (1024 ** 3)\n",
    "print(f\"Free space in {directory_to_check}: {free_space_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0f638-3c44-4a00-81ed-9d92134357dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
